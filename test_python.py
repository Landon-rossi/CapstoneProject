# -*- coding: utf-8 -*-
"""BigDataProject1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6Hrit9MrXT3uPdKwiZOE5dkCLuEGJzc

# Part 1
"""

!pip -q install umap-learn

# Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import RobustScaler, MaxAbsScaler
from sklearn.decomposition import PCA, SparsePCA
from sklearn.manifold import TSNE, LocallyLinearEmbedding
from sklearn.neighbors import NearestNeighbors
from umap import UMAP

# Function definitions

def normalize_data(data):
  std = StandardScaler().fit_transform(data)
  mmx = MinMaxScaler().fit_transform(data)
  rob = RobustScaler().fit_transform(data)
  mab = MaxAbsScaler().fit_transform(data)

  std = pd.DataFrame(std, columns=data.columns)
  mmx = pd.DataFrame(mmx, columns=data.columns)
  rob = pd.DataFrame(rob, columns=data.columns)
  mab = pd.DataFrame(mab, columns=data.columns)

  return [std, mmx, rob, mab]

def doPCA(data):
  pca = PCA(n_components=2)
  newData = pca.fit_transform(data)
  return newData

def doSparcePCA(data):
  spca = SparsePCA(n_components=2)
  newData = spca.fit_transform(data)
  return newData

def doTSNE(data, perplexity, init):
  tsne = TSNE(perplexity=perplexity, init=init)
  newData = tsne.fit_transform(data)
  return newData

def doUMAP(data, neighbor_size, init):
  umap = UMAP(n_neighbors=neighbor_size, init=init)
  newData = umap.fit_transform(data)
  return newData

def doLLE(data, neighbor_size, method):
  lle = LocallyLinearEmbedding(n_neighbors=neighbor_size, method=method)
  newData = lle.fit_transform(data)
  return newData

def visualize_data(data, norm, method, y):
  title = method
  plt.figure(figsize=(8, 6))
  colors = ['navy', 'turquoise', 'darkorange']
  for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):
      plt.scatter(data[y == i, 0], data[y == i, 1], color=color, alpha=.8, lw=2,
                  label=target_name)
  plt.legend(loc='best', shadow=False, scatterpoints=1)
  plt.title(f"{title} of "f"{norm} Iris Dataset")
  plt.show()

def localityConcentrationRatio(data, newData, k=10):
  nbrs  = NearestNeighbors(n_neighbors=k).fit(data)
  distances, ndx = nbrs.kneighbors(data)

  nbrsNew = NearestNeighbors(n_neighbors=k).fit(newData)
  distancesNew, ndxNew = nbrsNew.kneighbors(newData)

  sum = 0
  n = data.shape[0]

  for i in range(n):
    Ai = set(ndx[i])
    Bi = set(ndxNew[i])
    intersect = Ai.intersection(Bi)
    sum += len(intersect) / k

  return sum / n

# Load iris dataset
iris = load_iris()
data = iris.data
df = pd.DataFrame(data, columns=iris.feature_names)

normalized_dfs = normalize_data(df)

# Obtain dimension reduction results
pca_dfs = []
spca_dfs = []
tsne_dfs = []
umap_dfs = []
lle_dfs = []

for i in range(len(normalized_dfs)):
  pca_dfs.append(doPCA(normalized_dfs[i]))
  spca_dfs.append(doSparcePCA(normalized_dfs[i]))
  tsne_dfs.append(doTSNE(normalized_dfs[i], 5, 'pca'))
  umap_dfs.append(doUMAP(normalized_dfs[i], 5, 'random'))
  lle_dfs.append(doLLE(normalized_dfs[i], 5, 'standard'))

# Compare visualizations for PCA
norms = ['StandardScaler', 'MinMaxScaler', 'RobustScaler', 'MaxAbsScaler']

for i in range(len(pca_dfs)):
  visualize_data(pca_dfs[i], norm=norms[i], method='PCA', y=iris.target)

# Compare visualizations for Sparse PCA

for i in range(len(spca_dfs)):
  visualize_data(spca_dfs[i], norm=norms[i], method='Sparse PCA', y=iris.target)

# Compare visualzations for t-SNE

for i in range(len(tsne_dfs)):
  visualize_data(tsne_dfs[i], norm=norms[i], method='t-SNE', y=iris.target)

# Compare visualizations for UMAP

for i in range(len(umap_dfs)):
  visualize_data(umap_dfs[i], norm=norms[i], method='UMAP', y=iris.target)

# Compare visualizations for Locally Linear Embedding

for i in range(len(lle_dfs)):
  visualize_data(lle_dfs[i], norm=norms[i], method='LLE', y=iris.target)

# Calculate Locality Concentration Ratios

for i in range(len(norms)):
  print('\n' + norms[i], "data locality concentration ratios:")
  print("PCA :", localityConcentrationRatio(normalized_dfs[i], pca_dfs[i]))
  print("SPCA:", localityConcentrationRatio(normalized_dfs[i], spca_dfs[i]))
  print("TSNE:", localityConcentrationRatio(normalized_dfs[i], tsne_dfs[i]))
  print("UMAP:", localityConcentrationRatio(normalized_dfs[i], umap_dfs[i]))
  print("LLE :", localityConcentrationRatio(normalized_dfs[i], lle_dfs[i]))

"""# Part 2"""

# Commented out IPython magic to ensure Python compatibility.
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
import seaborn as sns
from scipy.stats import zscore
from sklearn.cluster import DBSCAN

iris = load_iris()
data = iris.data
# %matplotlib inline

import os
colab=1
if colab==1:
  from google.colab import drive
  drive.mount('/content/drive', force_remount=True)
  current_folder='DSC 43C8/Projects/Project 1'
  dest_folder='/content/drive/My Drive/'+ current_folder
  os.chdir(dest_folder)
  print('\n Current path: ' + os.getcwd())

drugs = pd.read_csv('GDSC_IC50.csv')
drugs.head()

drugs_data = drugs.dropna()
drugs_data.set_index('Unnamed: 0', inplace=True)
drugs_data.head()

Means = drugs_data.mean(axis=1)
threshold = Means.median()
binary_labels = (Means > threshold).astype(int)
drugs_data.insert(0, 'Binary_Label', binary_labels)
drugs_data.head()

pca = PCA(n_components=2)
pca_drug = pca.fit_transform(drugs_data)

pca = PCA(n_components=2)
pca_result = pca.fit_transform(drugs_data)
pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])

pca_df['zscore_PC1'] = zscore(pca_df['PC1'])
pca_df['zscore_PC2'] = zscore(pca_df['PC2'])
threshold = 3
outliers = pca_df[(pca_df['zscore_PC1'].abs() > threshold) | (pca_df['zscore_PC2'].abs() > threshold)]
plt.scatter(pca_df['PC1'], pca_df['PC2'], c=binary_labels, cmap='coolwarm', s=60, label='Data', alpha=0.6)
plt.scatter(outliers['PC1'], outliers['PC2'], color='orange', edgecolor='black', s=100, label='Outlier', marker='X')
plt.title('PCA of Drug Sensitivity Data')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()
print(f"Number of outliers detected: {len(outliers)}")

def embed(data):
    embeddings = {}
    pca = PCA(n_components=20)
    df_pca = pca.fit_transform(data)

    tsne = TSNE(n_components=2, perplexity=5, random_state=0)
    embeddings['t-SNE'] = tsne.fit_transform(df_pca)

    umap_model = UMAP(n_neighbors=10, init='spectral')
    embeddings['UMAP'] = umap_model.fit_transform(df_pca)

    lle = LocallyLinearEmbedding(n_components=2, random_state=0)
    embeddings['LLE'] = lle.fit_transform(df_pca)

    return embeddings

embeddings = embed(drugs_data)

def plot_embeddings(embeddings, data):
    fig, axs = plt.subplots(1, 3, figsize=(18, 6))

    for i, (key, embedding) in enumerate(embeddings.items()):
        sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=data['Binary_Label'], palette="coolwarm", s=60, ax=axs[i])
        axs[i].set_title(f'{key}')

    plt.show()


plot_embeddings(embeddings, drugs_data)

embeddings

def apply_dbscan(embeddings):
    for key, embedding in embeddings.items():
      if key == 't-SNE':
        dbscan = DBSCAN(eps=3, min_samples=2)
        clusters = dbscan.fit_predict(embedding)
        plt.figure(figsize=(6, 4))
        sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=clusters, palette="coolwarm", s=60)
        plt.title(f'DBSCAN for {key}')
        plt.show()
      elif key == 'UMAP':
        dbscan = DBSCAN(eps=0.25, min_samples=3)
        clusters = dbscan.fit_predict(embedding)
        plt.figure(figsize=(6, 4))
        sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=clusters, palette="coolwarm", s=60)
        plt.title(f'DBSCAN for {key}')
        plt.show()
      elif key == 'LLE':
        dbscan = DBSCAN(eps=0.01, min_samples=3)
        clusters = dbscan.fit_predict(embedding)
        plt.figure(figsize=(6, 4))
        sns.scatterplot(x=embedding[:, 0], y=embedding[:, 1], hue=clusters, palette="coolwarm", s=60)
        plt.title(f'DBSCAN for {key}')
        plt.show()
apply_dbscan(embeddings)

"""Upon examining the DBSCAN between the three, there is a clear differentiation between the three methods. For T-SNE it appears to have lots of clearly defined clusters. This can allow for specific cluster recognization and accurate predictions. For UMAP there are less clusters than T-SNE but there are still very distinct clusters which allow for efficient prediction. For LLE outliers seem to skew the efficiency of DBSCAN although it handles them into specific clusters. There are 3 clusters one for normal data a lower outlier cluster and a general outlier cluster.

# Part 3
"""

!pip -q install phate

from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from phate import PHATE

df = pd.read_csv('PatentCleanData.csv')
df.set_index('Patent_Num', inplace=True)
df

scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df)
df_scaled = pd.DataFrame(df_scaled, columns=df.columns)
df_scaled.head()

pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=df['classID'], palette="coolwarm", s=60)
plt.title('PCA of Patent Data')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

def apply_embedding_methods(df):
    X = random_sample.drop(columns=['classID'])

    pca = PCA(n_components=8)
    X_pca = pca.fit_transform(X)

    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    tsne_result = tsne.fit_transform(X_pca)

    umap_model = UMAP(n_neighbors=10, init='spectral')
    umap_result = umap_model.fit_transform(X_pca)

    return tsne_result, umap_result

def perform_kmeans_clustering(embeddings, df):
    metrics = {}

    for key, embedding in embeddings.items():
        kmeans = KMeans(n_clusters=2, random_state=42)
        labels = kmeans.fit_predict(embedding)

        ari = adjusted_rand_score(random_sample['classID'], labels)
        silhouette = silhouette_score(embedding, labels)
        calinski_harabasz = calinski_harabasz_score(embedding, labels)
        davies_bouldin = davies_bouldin_score(embedding, labels)

        metrics[key] = {
            'Adjusted Rand Index': ari,
            'Silhouette Score': silhouette,
            'Calinski-Harabasz Score': calinski_harabasz,
            'Davies-Bouldin Score': davies_bouldin
        }

    return metrics

random_sample = df_scaled.sample(n=1000, random_state=22)
tsne_result, umap_result = apply_embedding_methods(random_sample)
embeddings = {'t-SNE': tsne_result, 'UMAP': umap_result}
metrics = perform_kmeans_clustering(embeddings, df_scaled)

for key, metric in metrics.items():
    print(f"{key} Clustering Metrics:")
    for metric_name, value in metric.items():
        print(f"{metric_name}: {value:.4f}")
    print()

def doPHATE(original_df):
  labels = original_df['classID']
  data = original_df.drop(columns=['classID'])

  phate_operator = PHATE(verbose=0, knn=5, decay=10)
  newData = phate_operator.fit_transform(data)
  phate_df = pd.DataFrame(newData, columns=['PHATE1', 'PHATE2'])
  phate_df['classID'] = labels.reset_index(drop=True)

  return phate_df

phate_df = doPHATE(random_sample)

plt.figure(figsize=(10, 6))
unique_labels = phate_df['classID'].unique()
colors = ['darkorange', 'navy']
for i, label in enumerate(unique_labels):
  plt.scatter(phate_df[phate_df['classID'] == label]["PHATE1"],
              phate_df[phate_df['classID'] == label]["PHATE2"],
              marker='o', color=colors[i], alpha=0.2, label=f'Class {label}')
plt.title('PHATE of Patent Data')
plt.xlabel('PHATE1')
plt.ylabel('PHATE2')
plt.legend()
plt.show()

phate_df = phate_df.drop(columns=['classID'])

embeddings = {'PHATE': phate_df}
metrics = perform_kmeans_clustering(embeddings, df_scaled)
for key, metric in metrics.items():
    print(f"{key} Clustering Metrics:")
    for metric_name, value in metric.items():
        print(f"{metric_name}: {value:.4f}")

def stackingPCAtSNE(df, n_components_pca, perplex_tsne, init_tsne):
  labels = df['classID']
  df = df.drop(columns=['classID'])

  x_pca = PCA(n_components=n_components_pca).fit_transform(df)
  x_tsne = TSNE(perplexity=perplex_tsne, init=init_tsne).fit_transform(x_pca)

  tsne_df = pd.DataFrame(x_tsne, columns=['Component 1', 'Component 2'])
  tsne_df['classID'] = labels.reset_index(drop=True)

  return tsne_df

pca_tsne_df = stackingPCAtSNE(random_sample, 2, 15, 'pca')

plt.figure(figsize=(10, 6))
unique_labels = pca_tsne_df['classID'].unique()
colors = ['darkorange', 'navy']
for i, label in enumerate(unique_labels):
  plt.scatter(pca_tsne_df[pca_tsne_df['classID'] == label]["Component 1"],
              pca_tsne_df[pca_tsne_df['classID'] == label]["Component 2"],
              marker='o', color=colors[i], alpha=0.2, label=f'Class {label}')
plt.title('t-SNE of Patent Data in PCA Space')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend()
plt.show()

X = pca_tsne_df.drop(columns=['classID'])
y = pca_tsne_df['classID']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = svm.SVC(kernel='rbf')
clf.fit(X_train, y_train)
pred_labs = clf.predict(X_test)

def visualize_cm(true, pred):
  cm = confusion_matrix(true, pred)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  disp.plot()
  plt.show()

print("Accuracy of SVM on tSNE/PCA data:", accuracy_score(y_test, pred_labs))
print("\nConfusion Matrix:")
visualize_cm(y_test, pred_labs)